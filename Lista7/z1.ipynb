{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# podział na train i test, przygotowanie danych\n",
    "import numpy as np\n",
    "X_train_parabolic = np.linspace(-50, 50, 26)\n",
    "X_train_sin = np.linspace(0, 2, 21)\n",
    "Y_train_parabolic = X_train_parabolic**2\n",
    "Y_train_sin = np.sin((3*np.pi/2) * X_train_sin)\n",
    "\n",
    "X_test_parabolic = np.linspace(-50, 50, 101)\n",
    "X_test_sin = np.linspace(0, 2, 161)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sinus\n",
    "Testowałem różne ustawienia, duży wpływ miało dostawienie kolejnej warstwy oraz ustawienie funkcji aktywacji na *tanh*. Sieć dość szybko minimalizowała błąd, dlatego zmniejszyłem liczbę iteracji i zwiększyłem *learning_rate*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stworzenie modelu, odpowiednie warstwy\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "fig = plt.figure()\n",
    "ax_1 = fig.add_subplot(2, 1, 1)\n",
    "ax_1.set_title('Oryginalna funkcja')\n",
    "ax_1.scatter(X_train_sin, Y_train_sin)\n",
    "ax_2 = fig.add_subplot(2, 1, 2, sharex=ax_1)\n",
    "fig.tight_layout(pad=2.0)\n",
    "plt.ion()\n",
    "fig.canvas.draw()\n",
    "\n",
    "class PredictionCallback(tf.keras.callbacks.Callback):    \n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    if epoch % 100 == 0:\n",
    "        Y_predicted_sin = self.model.predict(X_test_sin)\n",
    "        ax_2.clear()\n",
    "        ax_2.title.set_text('Aproksymacja')\n",
    "        ax_2.scatter(X_test_sin, Y_predicted_sin)\n",
    "        ax_2.set_xlabel(f\"epoch = {epoch}, mse = {logs.get('loss')}\") \n",
    "        fig.canvas.draw()\n",
    "    \n",
    "hidden_dim = 10\n",
    "output_dim = 1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(hidden_dim, activation=tf.nn.tanh))\n",
    "model.add(Dense(hidden_dim, activation=tf.nn.tanh))\n",
    "model.add(Dense(output_dim, activation=tf.nn.tanh))\n",
    "\n",
    "model.compile(optimizer=optimizers.SGD(0.3),\n",
    "              loss=tf.keras.losses.mse)\n",
    "\n",
    "# trenowanie, funkcja fit\n",
    "model.fit(X_train_sin, Y_train_sin, epochs=2000\n",
    "          , callbacks=[PredictionCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parabola\n",
    "Trudno było dobrać parametry w taki sposób, by funkcja w ogóle zbiegała. Podczas wielu testów, początkowo sieć minimalizowała błąd, ale po osiągnięciu mse $\\approx$ 600000 nie było widać poprawy. Dopiero wyłączenie funkcji aktywacji (albo zmiana na ```tf.keras.activations.linear```) oraz zmiana *SGD* na *Adam* pozwoliła na poprawną aproksymację. Przy obecnych parametrach, gdybym zmienił z powrotem *Adam* na *SGD*, przybliżenie w ogóle by nie dawało poprawnych rezulatów, a mse = nan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax_1 = fig.add_subplot(2, 1, 1)\n",
    "ax_1.set_title('Oryginalna funkcja')\n",
    "ax_1.scatter(X_train_parabolic, Y_train_parabolic)\n",
    "ax_2 = fig.add_subplot(2, 1, 2, sharex=ax_1)\n",
    "fig.tight_layout(pad=2.0)\n",
    "plt.ion()\n",
    "fig.canvas.draw()\n",
    "\n",
    "class PredictionCallback(tf.keras.callbacks.Callback):    \n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    if epoch % 1000 == 0:\n",
    "        Y_predicted_parabolic = self.model.predict(X_test_parabolic)\n",
    "        ax_2.clear()\n",
    "        ax_2.title.set_text('Aproksymacja')\n",
    "        ax_2.scatter(X_test_parabolic, Y_predicted_parabolic)\n",
    "        ax_2.set_xlabel(f\"epoch = {epoch}, mse = {logs.get('loss')}\") \n",
    "        fig.canvas.draw()\n",
    "    \n",
    "hidden_dim = 10\n",
    "output_dim = 1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(hidden_dim, activation=tf.nn.sigmoid))\n",
    "model.add(Dense(hidden_dim))\n",
    "model.add(Dense(output_dim))\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=0.01),\n",
    "              loss=tf.keras.losses.mse)\n",
    "\n",
    "# trenowanie, funkcja fit\n",
    "model.fit(X_train_parabolic, Y_train_parabolic, epochs=10000\n",
    "          , callbacks=[PredictionCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
